{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Pc\\_netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhaithambadereddine-dz\u001b[0m (\u001b[33mhaithambadereddine-dz-badereddine-haitham\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Pc\\Desktop\\model\\wandb\\run-20250418_192924-6cq2xo0p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/haithambadereddine-dz-badereddine-haitham/uncategorized/runs/6cq2xo0p' target=\"_blank\">peach-eon-1</a></strong> to <a href='https://wandb.ai/haithambadereddine-dz-badereddine-haitham/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/haithambadereddine-dz-badereddine-haitham/uncategorized' target=\"_blank\">https://wandb.ai/haithambadereddine-dz-badereddine-haitham/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/haithambadereddine-dz-badereddine-haitham/uncategorized/runs/6cq2xo0p' target=\"_blank\">https://wandb.ai/haithambadereddine-dz-badereddine-haitham/uncategorized/runs/6cq2xo0p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact run_qrvwn3vd_model:v49, 197.82MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:2:38.8\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "run = wandb.init()\n",
    "artifact = run.use_artifact('ms-ouareth1/ultralytics/run_qrvwn3vd_model:v49', type='model')\n",
    "artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting video processing...\n",
      "Processing complete. Output saved to detection_output.mp4\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "# Set paths - using raw strings to handle Windows paths\n",
    "MODEL_WEIGHTS = r\"C:\\Users\\Pc\\Desktop\\model\\artifacts\\run_qrvwn3vd_model-v49\\epoch49.pt\"\n",
    "VIDEO_PATH = r\"C:\\Users\\Pc\\Desktop\\model\\PPE_Part1.mp4\"\n",
    "OUTPUT_PATH = \"detection_output.mp4\"\n",
    "\n",
    "def process_video_yolov8(model, video_path, output_path, conf_thresh=0.5):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "    \n",
    "    # Get video properties\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Set up output video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # Perform inference\n",
    "        results = model(frame, verbose=False)\n",
    "        \n",
    "        # Draw boxes\n",
    "        for result in results:\n",
    "            boxes = result.boxes\n",
    "            for box in boxes:\n",
    "                if box.conf.item() > conf_thresh:\n",
    "                    x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                    label = f\"{model.names[box.cls.item()]} {box.conf.item():.2f}\"\n",
    "                    cv2.putText(frame, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "        \n",
    "        # Write and display\n",
    "        out.write(frame)\n",
    "        cv2.imshow('YOLOv8 Detection', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load model\n",
    "    model = YOLO(MODEL_WEIGHTS)\n",
    "    \n",
    "    # Process video\n",
    "    print(\"Starting video processing...\")\n",
    "    process_video_yolov8(model, VIDEO_PATH, OUTPUT_PATH)\n",
    "    print(f\"Processing complete. Output saved to {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLOv8 model...\n",
      "Starting video processing with alert system...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 106\u001b[0m\n\u001b[0;32m    103\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(MODEL_WEIGHTS)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting video processing with alert system...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 106\u001b[0m \u001b[43mprocess_video_with_alerts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVIDEO_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUTPUT_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing complete. Output saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUTPUT_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[16], line 88\u001b[0m, in \u001b[0;36mprocess_video_with_alerts\u001b[1;34m(model, video_path, output_path, conf_thresh)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m alert_triggered \u001b[38;5;129;01mand\u001b[39;00m (current_time \u001b[38;5;241m-\u001b[39m last_alert_time) \u001b[38;5;241m>\u001b[39m ALERT_FLASH_DURATION:\n\u001b[0;32m     87\u001b[0m     alert_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 88\u001b[0m     \u001b[43malert_system\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malert_count\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m     last_alert_time \u001b[38;5;241m=\u001b[39m current_time\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Write and display\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[16], line 31\u001b[0m, in \u001b[0;36malert_system\u001b[1;34m(frame, alert_count)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Show the alert frame briefly\u001b[39;00m\n\u001b[0;32m     30\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYOLOv8 Detection\u001b[39m\u001b[38;5;124m'\u001b[39m, flash_frame)\n\u001b[1;32m---> 31\u001b[0m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mALERT_FLASH_DURATION\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Display alert counter\u001b[39;00m\n\u001b[0;32m     34\u001b[0m cv2\u001b[38;5;241m.\u001b[39mputText(frame, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlerts: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00malert_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m40\u001b[39m), \n\u001b[0;32m     35\u001b[0m            cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m1\u001b[39m, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import winsound  # For Windows sound alerts\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "MODEL_WEIGHTS = Path(r\"C:\\Users\\Pc\\Desktop\\model\\artifacts\\run_qrvwn3vd_model-v49\\epoch49.pt\")\n",
    "VIDEO_PATH = Path(r\"C:\\Users\\Pc\\Desktop\\model\\PPE_Part1.mp4\")\n",
    "OUTPUT_PATH = \"detection_output.mp4\"\n",
    "ALERT_CLASSES = [\"Non-Helmet\", \"no-vest\", \"bare-arms\"]  # Classes that should trigger alerts\n",
    "ALERT_BEEP_FREQ = 1000  # Frequency in Hz\n",
    "ALERT_BEEP_DUR = 200  # Duration in ms\n",
    "ALERT_FLASH_COLOR = (0, 0, 255)  # Red flashing border\n",
    "ALERT_FLASH_DURATION = 0.01  # Seconds\n",
    "\n",
    "def alert_system(frame, alert_count):\n",
    "    \"\"\"Visual and audio alert system\"\"\"\n",
    "    # Audio alert\n",
    "    winsound.Beep(ALERT_BEEP_FREQ, ALERT_BEEP_DUR)\n",
    "    \n",
    "    # Visual alert (flashing border)\n",
    "    flash_frame = frame.copy()\n",
    "    border_width = 30\n",
    "    cv2.rectangle(flash_frame, (0, 0), (flash_frame.shape[1], flash_frame.shape[0]), \n",
    "                 ALERT_FLASH_COLOR, border_width)\n",
    "    \n",
    "    # Show the alert frame briefly\n",
    "    cv2.imshow('YOLOv8 Detection', flash_frame)\n",
    "    cv2.waitKey(int(ALERT_FLASH_DURATION * 1000))\n",
    "    \n",
    "    # Display alert counter\n",
    "    cv2.putText(frame, f\"Alerts: {alert_count}\", (20, 40), \n",
    "               cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "def process_video_with_alerts(model, video_path, output_path, conf_thresh=0.5):\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "    \n",
    "    # Video properties\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Output video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "    \n",
    "    alert_count = 0\n",
    "    last_alert_time = 0\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # Inference\n",
    "        results = model(frame, verbose=False)\n",
    "        \n",
    "        # Process detections\n",
    "        alert_triggered = False\n",
    "        for result in results:\n",
    "            boxes = result.boxes\n",
    "            for box in boxes:\n",
    "                if box.conf.item() > conf_thresh:\n",
    "                    cls_name = model.names[box.cls.item()]\n",
    "                    if cls_name.lower() in ALERT_CLASSES:\n",
    "                        alert_triggered = True\n",
    "                    \n",
    "                    # Draw bounding box\n",
    "                    x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "                    color = (0, 255, 0)  # Green for normal detections\n",
    "                    if cls_name.lower() in ALERT_CLASSES:\n",
    "                        color = (0, 0, 255)  # Red for alert classes\n",
    "                    \n",
    "                    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "                    label = f\"{cls_name} {box.conf.item():.2f}\"\n",
    "                    cv2.putText(frame, label, (x1, y1-10), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "        \n",
    "        # Trigger alert if needed (with cooldown)\n",
    "        current_time = time.time()\n",
    "        if alert_triggered and (current_time - last_alert_time) > ALERT_FLASH_DURATION:\n",
    "            alert_count += 1\n",
    "            alert_system(frame, alert_count)\n",
    "            last_alert_time = current_time\n",
    "        \n",
    "        # Write and display\n",
    "        out.write(frame)\n",
    "        cv2.imshow('YOLOv8 Detection', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading YOLOv8 model...\")\n",
    "    model = YOLO(MODEL_WEIGHTS)\n",
    "    \n",
    "    print(\"Starting video processing with alert system...\")\n",
    "    process_video_with_alerts(model, VIDEO_PATH, OUTPUT_PATH)\n",
    "    \n",
    "    print(f\"Processing complete. Output saved to {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLOv8 model...\n",
      "Starting video processing with PPE counter...\n",
      "\n",
      "Final Detection Counts:\n",
      "Non-Helmet: 0\n",
      "no-vest: 730\n",
      "bare-arms: 5\n",
      "\n",
      "Processing complete. Output saved to detection_output.mp4\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "MODEL_WEIGHTS = Path(r\"C:\\Users\\Pc\\Desktop\\model\\artifacts\\run_qrvwn3vd_model-v49\\epoch49.pt\")\n",
    "VIDEO_PATH = Path(r\"C:\\Users\\Pc\\Desktop\\model\\PPE_Part1.mp4\")\n",
    "OUTPUT_PATH = \"detection_output.mp4\"\n",
    "ALERT_CLASSES = [\"Non-Helmet\", \"no-vest\", \"bare-arms\"]  # Classes that should trigger alerts\n",
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "\n",
    "def process_video_with_counter(model, video_path, output_path):\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "    \n",
    "    # Video properties\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Output video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "    \n",
    "    # Initialize counters\n",
    "    class_counts = {cls: 0 for cls in ALERT_CLASSES}\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # Reset counts for current frame\n",
    "        current_frame_counts = {cls: 0 for cls in ALERT_CLASSES}\n",
    "        \n",
    "        # Inference\n",
    "        results = model(frame, verbose=False)\n",
    "        \n",
    "        # Process detections\n",
    "        for result in results:\n",
    "            boxes = result.boxes\n",
    "            for box in boxes:\n",
    "                if box.conf.item() > CONFIDENCE_THRESHOLD:\n",
    "                    cls_name = model.names[box.cls.item()].lower()\n",
    "                    \n",
    "                    # Update counts if class is in our alert classes\n",
    "                    if cls_name in ALERT_CLASSES:\n",
    "                        class_counts[cls_name] += 1\n",
    "                        current_frame_counts[cls_name] += 1\n",
    "                    \n",
    "                    # Draw bounding box\n",
    "                    x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "                    color = (0, 255, 0)  # Green for normal detections\n",
    "                    if cls_name in ALERT_CLASSES:\n",
    "                        color = (0, 0, 255)  # Red for alert classes\n",
    "                    \n",
    "                    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "                    label = f\"{cls_name} {box.conf.item():.2f}\"\n",
    "                    cv2.putText(frame, label, (x1, y1-10), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "        \n",
    "        # Display current frame counts\n",
    "        y_offset = 40\n",
    "        for cls, count in current_frame_counts.items():\n",
    "            if count > 0:\n",
    "                text = f\"{cls}: {count} (Total: {class_counts[cls]})\"\n",
    "                cv2.putText(frame, text, (20, y_offset), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "                y_offset += 30\n",
    "        \n",
    "        # Write and display\n",
    "        out.write(frame)\n",
    "        cv2.imshow('YOLOv8 Detection - PPE Counter', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    # Print final counts\n",
    "    print(\"\\nFinal Detection Counts:\")\n",
    "    for cls, count in class_counts.items():\n",
    "        print(f\"{cls}: {count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading YOLOv8 model...\")\n",
    "    model = YOLO(MODEL_WEIGHTS)\n",
    "    \n",
    "    print(\"Starting video processing with PPE counter...\")\n",
    "    process_video_with_counter(model, VIDEO_PATH, OUTPUT_PATH)\n",
    "    \n",
    "    print(f\"\\nProcessing complete. Output saved to {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPU:  1\n",
      "GPU Name:  NVIDIA GeForce RTX 3050 Ti Laptop GPU\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Number of GPU: \", torch.cuda.device_count())\n",
    "print(\"GPU Name: \", torch.cuda.get_device_name())\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
